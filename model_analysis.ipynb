{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f550ce-d11c-49d5-b675-427a9a459b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import sympy as sp\n",
    "import math\n",
    "# !pip3 install git+https://github.com/Fangyh09/pytorch-receptive-field.git\n",
    "from torch_receptive_field import receptive_field\n",
    "from torchinfo import summary\n",
    "\n",
    "import models as cifar_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b10daeed-81b6-48f4-a209-ad524907680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Layer: convBlock1\n",
      "- SubLayer: Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "-------------------------------- no. of channels: 32 | feature map dim: 32x32\n",
      "- SubLayer: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "- SubLayer: Dropout(p=0.1, inplace=False)\n",
      "- Layer: convBlock2\n",
      "- SubLayer: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "-------------------------------- no. of channels: 32 | feature map dim: 32x32\n",
      "- SubLayer: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "- SubLayer: Dropout(p=0.1, inplace=False)\n",
      "- Layer: convBlock3\n",
      "- SubLayer: Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "-------------------------------- no. of channels: 16 | feature map dim: 32x32\n",
      "- Layer: pool1\n",
      "-------------------------------- no. of channels: 16 | feature map dim: 16x16\n",
      "\n",
      "- Layer: convBlock4\n",
      "- SubLayer: Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "-------------------------------- no. of channels: 32 | feature map dim: 16x16\n",
      "- SubLayer: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "- SubLayer: Dropout(p=0.1, inplace=False)\n",
      "- Layer: convBlock5\n",
      "- SubLayer: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "-------------------------------- no. of channels: 32 | feature map dim: 16x16\n",
      "- SubLayer: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "- SubLayer: Dropout(p=0.1, inplace=False)\n",
      "- Layer: convBlock6\n",
      "- SubLayer: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "-------------------------------- no. of channels: 32 | feature map dim: 16x16\n",
      "- SubLayer: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "- SubLayer: Dropout(p=0.1, inplace=False)\n",
      "- Layer: convBlock7\n",
      "- SubLayer: Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "-------------------------------- no. of channels: 16 | feature map dim: 16x16\n",
      "- Layer: pool2\n",
      "-------------------------------- no. of channels: 16 | feature map dim: 8x8\n",
      "\n",
      "- Layer: convBlock8\n",
      "- SubLayer: Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "-------------------------------- no. of channels: 32 | feature map dim: 8x8\n",
      "- SubLayer: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "- SubLayer: Dropout(p=0.1, inplace=False)\n",
      "- Layer: convBlock9\n",
      "- SubLayer: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "-------------------------------- no. of channels: 32 | feature map dim: 6x6\n",
      "- SubLayer: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "- SubLayer: Dropout(p=0.1, inplace=False)\n",
      "- Layer: convBlock10\n",
      "- SubLayer: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "-------------------------------- no. of channels: 32 | feature map dim: 4x4\n",
      "- SubLayer: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "- SubLayer: Dropout(p=0.1, inplace=False)\n",
      "- Layer: gap\n",
      "-------------------------------- no. of channels: 32 | feature map dim: 1x1\n",
      "\n",
      "- Layer: convBlock11\n",
      "- SubLayer: Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "-------------------------------- no. of channels: 10 | feature map dim: 1x1\n"
     ]
    }
   ],
   "source": [
    "model = cifar_models.NetBNv2()\n",
    "\n",
    "counter = 0\n",
    "# Loop through each layer of the model\n",
    "c_in, h_in, w_in = 3, 32, 32\n",
    "for name, layer in model.named_children():\n",
    "    # print(f\"- Layer details: {name} | k={layer.kernel_size} | s={layer.stride} | p={layer.padding}\")\n",
    "    print(f\"- Layer: {name}\")\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        out_channels = layer.out_channels\n",
    "        k, s, p = layer.kernel_size[0], layer.stride[0], layer.padding[0]\n",
    "        c_o, h_o, w_o = out_channels, math.floor((h_in + 2*p - k)/s + 1), math.floor((w_in + 2*p - k)/s + 1)\n",
    "        print(f\"-------------------------------- no. of channels: {c_o} | feature map dim: {h_o}x{w_o}\")\n",
    "        c_in, h_in, w_in = c_o, h_o, w_o\n",
    "    \n",
    "    elif isinstance(layer, nn.MaxPool2d):\n",
    "        k, s, p = layer.kernel_size, layer.stride, layer.padding\n",
    "        c_o, h_o, w_o = c_in, math.floor((h_in + 2*p - k)/s + 1), math.floor((w_in + 2*p - k)/s + 1)\n",
    "        print(f\"-------------------------------- no. of channels: {c_o} | feature map dim: {h_o}x{w_o}\\n\")\n",
    "        c_in, h_in, w_in = c_o, h_o, w_o\n",
    "\n",
    "    elif isinstance(layer, nn.AvgPool2d):\n",
    "        k, s, p = layer.kernel_size, layer.stride, layer.padding\n",
    "        c_o, h_o, w_o = c_in, math.floor((h_in + 2*p - k)/s + 1), math.floor((w_in + 2*p - k)/s + 1)\n",
    "        print(f\"-------------------------------- no. of channels: {c_o} | feature map dim: {h_o}x{w_o}\\n\")\n",
    "        c_in, h_in, w_in = c_o, h_o, w_o\n",
    "\n",
    "    elif isinstance(layer, nn.Sequential):\n",
    "        for sub_name, sub_layer in layer.named_children():\n",
    "            # print(f\"- SubLayer details: {sub_name} | k={sub_layer.kernel_size} | s={sub_layer.stride} | p={sub_layer.padding}\")\n",
    "            if isinstance(sub_layer, nn.ReLU):\n",
    "                continue\n",
    "            print(f\"- SubLayer: {sub_layer}\")\n",
    "            if isinstance(sub_layer, nn.Conv2d):\n",
    "                out_channels = sub_layer.out_channels\n",
    "                k, s, p = sub_layer.kernel_size[0], sub_layer.stride[0], sub_layer.padding[0]\n",
    "                c_o, h_o, w_o = out_channels, math.floor((h_in + 2*p - k)/s + 1), math.floor((w_in + 2*p - k)/s + 1)\n",
    "                print(f\"-------------------------------- no. of channels: {c_o} | feature map dim: {h_o}x{w_o}\")\n",
    "                c_in, h_in, w_in = c_o, h_o, w_o\n",
    "            \n",
    "            elif isinstance(sub_layer, nn.MaxPool2d):\n",
    "                k, s, p = sub_layer.kernel_size, sub_layer.stride, sub_layer.padding\n",
    "                c_o, h_o, w_o = c_in, math.floor((h_in + 2*p - k)/s + 1), math.floor((w_in + 2*p - k)/s + 1)\n",
    "                print(f\"-------------------------------- no. of channels: {c_o} | feature map dim: {h_o}x{w_o}\\n\")\n",
    "                c_in, h_in, w_in = c_o, h_o, w_o\n",
    "\n",
    "            elif isinstance(sub_layer, nn.AvgPool2d):\n",
    "                k, s, p = sub_layer.kernel_size, sub_layer.stride, sub_layer.padding\n",
    "                c_o, h_o, w_o = c_in, math.floor((h_in + 2*p - k)/s + 1), math.floor((w_in + 2*p - k)/s + 1)\n",
    "                print(f\"-------------------------------- no. of channels: {c_o} | feature map dim: {h_o}x{w_o}\\n\")\n",
    "                c_in, h_in, w_in = c_o, h_o, w_o\n",
    "    else:\n",
    "        print('layer not implemented')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba062f24-e9ed-4203-90b1-1e83f6989c48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "module Dropout not ok",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# receptive_field(model, input_size=(channels, H, W))\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mreceptive_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch_receptive_field/receptive_field.py:136\u001b[0m, in \u001b[0;36mreceptive_field\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m    133\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/tsai_erav2/session_8/s8_assignment_1/models.py:88\u001b[0m, in \u001b[0;36mNetBNv2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 88\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvBlock1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvBlock2(x)\n\u001b[1;32m     90\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvBlock3(x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1574\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1572\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1574\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1577\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch_receptive_field/receptive_field.py:82\u001b[0m, in \u001b[0;36mreceptive_field.<locals>.register_hook.<locals>.hook\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m     80\u001b[0m         receptive_field[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not ok\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(class_name))\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     84\u001b[0m receptive_field[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;66;03m# only one\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: module Dropout not ok"
     ]
    }
   ],
   "source": [
    "# receptive_field(model, input_size=(channels, H, W))\n",
    "receptive_field(model, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866b4b66-3f9f-42fb-805c-aea7122e89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(input_size=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a4a2b-19a8-40ea-b078-13d367f7231b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
